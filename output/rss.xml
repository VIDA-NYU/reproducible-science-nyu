<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Reproducible Science</title><link>https://reproduciblescience.org/</link><description>Promoting Open Science</description><atom:link rel="self" type="application/rss+xml" href="https://reproduciblescience.org/rss.xml"></atom:link><language>en</language><lastBuildDate>Fri, 20 Jan 2017 17:16:51 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>A Survey of Current Reproducibility Practices in Linguistics Publications</title><link>https://reproduciblescience.org/directory/repro-survey-linguistics/</link><dc:creator>NYU Reproducibility Working Group</dc:creator><description>&lt;p&gt;This project considers the role of reproducibility in increasing verification and accountability in linguistic research. An analysis of over 370 journal articles, dissertations, and grammars from a ten-year span is taken as a sample of current practices in the field. These are critiqued on the basis of transparency of data source, data collection methods, analysis, and storage. While we find examples of transparent reporting, much of the surveyed research does not include key metadata, methodological information, or citations that are resolvable to the data on which the analyses are based. This has implications for reproducibility and hence accountability, hallmarks of social science research which are currently under-represented in linguistic research.&lt;/p&gt;

  </description><category>reproducibility report</category><guid>https://reproduciblescience.org/directory/repro-survey-linguistics/</guid><pubDate>Wed, 18 Jan 2017 22:51:50 GMT</pubDate></item><item><title>Cancer reproducibility project releases first results</title><link>https://reproduciblescience.org/directory/cancer-repro-project-results/</link><dc:creator>NYU Reproducibility Working Group</dc:creator><description>&lt;p&gt;The Reproducibility Project: Cancer Biology launched in 2013 as an ambitious effort to scrutinize key findings in 50 cancer papers published in Nature, Science, Cell and other high-impact journals. It aims to determine what fraction of influential cancer biology studies are probably sound — a pressing question for the field. In 2012, researchers at the biotechnology firm Amgen in Thousand Oaks, California, announced that they had failed to replicate 47 of 53 landmark cancer papers2. That was widely reported, but Amgen has not identified the studies involved.&lt;/p&gt;

  </description><category>news article</category><category>reproducible paper</category><guid>https://reproduciblescience.org/directory/cancer-repro-project-results/</guid><pubDate>Wed, 18 Jan 2017 22:51:50 GMT</pubDate></item><item><title>Opening the Publication Process with Executable Research Compendia</title><link>https://reproduciblescience.org/directory/open-pub-executable-compendia/</link><dc:creator>NYU Reproducibility Working Group</dc:creator><description>&lt;p&gt;A strong movement towards openness has seized science. Open data and methods, open source software, Open Access, open reviews, and open research platforms provide the legal and technical solutions to new forms of research and publishing. However, publishing reproducible research is still not common practice. Reasons include a lack of incentives and a missing standardized infrastructure for providing research material such as data sets and source code together with a scientific paper. Therefore we first study fundamentals and existing approaches. On that basis, our key contributions are the identification of core requirements of authors, readers, publishers, curators, as well as preservationists and the subsequent description of an executable research compendium (ERC). It is the main component of a publication process providing a new way to publish and access computational research. ERCs provide a new standardisable packaging mechanism which combines data, software, text, and a user interface description. We discuss the potential of ERCs and their challenges in the context of user requirements and the established publication processes. We conclude that ERCs provide a novel potential to find, explore, reuse, and archive computer-based research.
&lt;/p&gt;

  </description><category>reproducible paper</category><guid>https://reproduciblescience.org/directory/open-pub-executable-compendia/</guid><pubDate>Mon, 16 Jan 2017 22:51:50 GMT</pubDate></item><item><title>Supporting Data Reproducibility at NCI Using the Provenance Capture System</title><link>https://reproduciblescience.org/directory/data-repro-nci/</link><dc:creator>NYU Reproducibility Working Group</dc:creator><description>&lt;p&gt;Scientific research is published in journals so that the research community is able to share knowledge and results, verify hypotheses, contribute evidence-based opinions and promote discussion. However, it is hard to fully understand, let alone reproduce, the results if the complex data manipulation that was undertaken to obtain the results are not clearly explained and/or the final data used is not available. Furthermore, the scale of research data assets has now exponentially increased to the point that even when available, it can be difficult to store and use these data assets. In this paper, we describe the solution we have implemented at the National Computational Infrastructure (NCI) whereby researchers can capture workflows, using a standards-based provenance representation. This provenance information, combined with access to the original dataset and other related information systems, allow datasets to be regenerated as needed which simultaneously addresses both result reproducibility and storage issues.
&lt;/p&gt;

  </description><category>reproducible paper</category><guid>https://reproduciblescience.org/directory/data-repro-nci/</guid><pubDate>Mon, 16 Jan 2017 22:51:50 GMT</pubDate></item><item><title>A manifesto for reproducible science</title><link>https://reproduciblescience.org/directory/manifesto-reproducible-science/</link><dc:creator>NYU Reproducibility Working Group</dc:creator><description>&lt;p&gt;Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.&lt;/p&gt;

  </description><category>reproducibility report</category><category>reproducible paper</category><guid>https://reproduciblescience.org/directory/manifesto-reproducible-science/</guid><pubDate>Tue, 10 Jan 2017 22:51:50 GMT</pubDate></item><item><title>Scientific papers need better feedback systems. Here's why</title><link>https://reproduciblescience.org/directory/scientific-papers-better-feedback/</link><dc:creator>NYU Reproducibility Working Group</dc:creator><description>&lt;p&gt;Somewhere between 65 and 90 per cent of biomedical literature is considered non-reproducible. This means that if you try to reproduce an experiment described in a given paper, 65 to 90 per cent of the time you won't get the same findings. We call this the reproducibility crisis. The issue became live thanks to a study by Glenn Begley, who ran the oncology department at Amgen, a pharmaceutical company. In 2011, Begley decided to try to reproduce findings in 53 foundational papers in oncology: highly cited papers published in the top journals. He was unable to reproduce 47 of them - 89 per cent.&lt;/p&gt;

&lt;!-- Bootstrap core JavaScript
    ================================================== --&gt;
    &lt;!-- Placed at the end of the document so the pages load faster --&gt;
    &lt;script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"&gt;&lt;/script&gt;
    &lt;script&gt;window.jQuery || document.write('&lt;script src="../../assets/js/vendor/jquery.min.js"&gt;&lt;\/script&gt;')&lt;/script&gt;
    &lt;script src="https://reproduciblescience.org/js/bootstrap.min.js"&gt;&lt;/script&gt;
    &lt;!-- IE10 viewport hack for Surface/desktop Windows 8 bug --&gt;
    &lt;script src="https://reproduciblescience.org/js/ie10-viewport-bug-workaround.js"&gt;&lt;/script&gt;
  </description><category>news article</category><guid>https://reproduciblescience.org/directory/scientific-papers-better-feedback/</guid><pubDate>Sun, 08 Jan 2017 22:51:50 GMT</pubDate></item><item><title>Lack of reproducibility triggers retractions of Nature Materials articles</title><link>https://reproduciblescience.org/directory/lack-repro-triggers-retractions/</link><dc:creator>NYU Reproducibility Working Group</dc:creator><description>&lt;p&gt;The authors of a highly cited 2015 paper in Nature Materials have retracted it, after being unable to reproduce some of the key findings. We’ve seen this kind of thing before, from another Nature journal, although in one case the News &amp;amp; Views article only earned a warning notice.&lt;/p&gt;

&lt;!-- Bootstrap core JavaScript
    ================================================== --&gt;
    &lt;!-- Placed at the end of the document so the pages load faster --&gt;
    &lt;script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"&gt;&lt;/script&gt;
    &lt;script&gt;window.jQuery || document.write('&lt;script src="../../assets/js/vendor/jquery.min.js"&gt;&lt;\/script&gt;')&lt;/script&gt;
    &lt;script src="https://reproduciblescience.org/js/bootstrap.min.js"&gt;&lt;/script&gt;
    &lt;!-- IE10 viewport hack for Surface/desktop Windows 8 bug --&gt;
    &lt;script src="https://reproduciblescience.org/js/ie10-viewport-bug-workaround.js"&gt;&lt;/script&gt;
  </description><category>news article</category><guid>https://reproduciblescience.org/directory/lack-repro-triggers-retractions/</guid><pubDate>Wed, 28 Dec 2016 05:12:00 GMT</pubDate></item><item><title>Leveraging Statistical Methods to Improve Validity and Reproducibility of Research Findings</title><link>https://reproduciblescience.org/directory/stats-methods-improve-repro/</link><dc:creator>NYU Reproducibility Working Group</dc:creator><description>&lt;p&gt;Scientific discoveries have the profound opportunity to impact the lives of patients. They can lead to advances in medical decision making when the findings are correct, or mislead when not. We owe it to our peers, funding sources, and patients to take every precaution against false conclusions, and to communicate our discoveries with accuracy, precision, and clarity. With the National Institutes of Health’s new focus on rigor and reproducibility, scientists are returning attention to the ideas of validity and reliability. At JAMA Psychiatry, we seek to publish science that leverages the power of statistics and contributes discoveries that are reproducible and valid. Toward that end, I provide guidelines for using statistical methods: the essentials, good practices, and advanced methods.&lt;/p&gt;

&lt;!-- Bootstrap core JavaScript
    ================================================== --&gt;
    &lt;!-- Placed at the end of the document so the pages load faster --&gt;
    &lt;script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"&gt;&lt;/script&gt;
    &lt;script&gt;window.jQuery || document.write('&lt;script src="../../assets/js/vendor/jquery.min.js"&gt;&lt;\/script&gt;')&lt;/script&gt;
    &lt;script src="https://reproduciblescience.org/js/bootstrap.min.js"&gt;&lt;/script&gt;
    &lt;!-- IE10 viewport hack for Surface/desktop Windows 8 bug --&gt;
    &lt;script src="https://reproduciblescience.org/js/ie10-viewport-bug-workaround.js"&gt;&lt;/script&gt;
  </description><category>reproducibility guidelines</category><guid>https://reproduciblescience.org/directory/stats-methods-improve-repro/</guid><pubDate>Wed, 28 Dec 2016 05:12:00 GMT</pubDate></item><item><title>Transparency, Reproducibility, and the Credibility of Economics Research</title><link>https://reproduciblescience.org/directory/transparency-repro-ecnomics/</link><dc:creator>NYU Reproducibility Working Group</dc:creator><description>&lt;p&gt;There is growing interest in enhancing research transparency and reproducibility in economics and other scientific fields. We survey existing work on these topics within economics, and discuss the evidence suggesting that publication bias, inability to replicate, and specification searching remain widespread in the discipline. We next discuss recent progress in this area, including through improved research design, study registration and pre-analysis plans, disclosure standards, and open sharing of data and materials, drawing on experiences in both economics and other social sciences. We discuss areas where consensus is emerging on new practices, as well as approaches that remain controversial, and speculate about the most effective ways to make economics research more credible in the future.&lt;/p&gt;

&lt;!-- Bootstrap core JavaScript
    ================================================== --&gt;
    &lt;!-- Placed at the end of the document so the pages load faster --&gt;
    &lt;script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"&gt;&lt;/script&gt;
    &lt;script&gt;window.jQuery || document.write('&lt;script src="../../assets/js/vendor/jquery.min.js"&gt;&lt;\/script&gt;')&lt;/script&gt;
    &lt;script src="https://reproduciblescience.org/js/bootstrap.min.js"&gt;&lt;/script&gt;
    &lt;!-- IE10 viewport hack for Surface/desktop Windows 8 bug --&gt;
    &lt;script src="https://reproduciblescience.org/js/ie10-viewport-bug-workaround.js"&gt;&lt;/script&gt;
  </description><category>reproducibility report</category><guid>https://reproduciblescience.org/directory/transparency-repro-ecnomics/</guid><pubDate>Thu, 22 Dec 2016 05:12:00 GMT</pubDate></item><item><title>The State of Reproducibility: 16 Advances from 2016</title><link>https://reproduciblescience.org/directory/state-of-repro-16-advances-from-2016/</link><dc:creator>NYU Reproducibility Working Group</dc:creator><description>&lt;p&gt;2016 saw a tremendous amount of discussion and development on the subject of scientific reproducibility. Were you able to keep up? If not, check out this list of 16 sources from 2016 to get you up to date for the new year! The reproducibility crisis in science refers to the difficulty scientists have faced in reproducing or replicating results from previously published scientific experiments. Although this crisis has existed in the scientific community for a very long time, it gained much more visibility in in the past few years. The terms “reproducibility crisis” and “replicability crisis” were coined in the early 2010s due to the growing awareness of the problem.&lt;/p&gt;

&lt;!-- Bootstrap core JavaScript
    ================================================== --&gt;
    &lt;!-- Placed at the end of the document so the pages load faster --&gt;
    &lt;script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"&gt;&lt;/script&gt;
    &lt;script&gt;window.jQuery || document.write('&lt;script src="../../assets/js/vendor/jquery.min.js"&gt;&lt;\/script&gt;')&lt;/script&gt;
    &lt;script src="https://reproduciblescience.org/js/bootstrap.min.js"&gt;&lt;/script&gt;
    &lt;!-- IE10 viewport hack for Surface/desktop Windows 8 bug --&gt;
    &lt;script src="https://reproduciblescience.org/js/ie10-viewport-bug-workaround.js"&gt;&lt;/script&gt;
  </description><category>popular news</category><guid>https://reproduciblescience.org/directory/state-of-repro-16-advances-from-2016/</guid><pubDate>Wed, 21 Dec 2016 05:12:00 GMT</pubDate></item></channel></rss>